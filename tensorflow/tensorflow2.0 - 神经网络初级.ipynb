{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "### 1. 一些概念\n",
    "机器学习的最终目的是找到一组良好的参数θ,使得θ表示的数学模型能够很好地从训练集中学到映射关系，从而利用训练好的去预测新样本。\n",
    "\n",
    "神经网络属于机器学习的一个研究分支，它特指利用多个神经元去参数化映射函数的模型。\n",
    "\n",
    "- 偏置 b\n",
    "- 权值 w\n",
    "- 净活性值 z 即净输入，输入数据x的加权和\n",
    "- 激活函数 σ\n",
    "- 活性值 净活性值通过激活函数激活后的值\n",
    "\n",
    "#### 激活函数\n",
    "- 可以是阶跃函数；符号函数等，但是这类函数在z=0处不连续，其他位置导数为0，无法利用梯度下降法进行参数优化\n",
    "- 感知机模型的不可导特性严重约束了它的潜力。现代深度学习在感知机的基础上，将不连续的阶跃激活函数换成了其他平滑连续激活函数，并通过堆叠多层网络层来增强网络的表达能力。\n",
    "\n",
    "#### 全连接层\n",
    "每个输出节点与全部的输入节点相连接\n",
    "\n",
    "### 2. 张量方式实现感知机神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成输入数据，2个样本，784个特征\n",
    "x = tf.random.normal([2, 784])\n",
    "\n",
    "# 生成权值向量，256个神经元\n",
    "# truncated_normal生成的值遵循具有指定平均值和标准偏差的正态分布\n",
    "# 不同之处在于其平均值大于 2 个标准差的值将被丢弃并重新选择.\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev = 0.1))\n",
    "\n",
    "# 生成偏置向量\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "\n",
    "# 线性变换\n",
    "o1 = tf.matmul(x, w1) + b1\n",
    "\n",
    "# 激活函数\n",
    "o1 = tf.nn.relu(o1)\n",
    "\n",
    "# 得到的活性值为2个样本，每个样本256个输出\n",
    "o1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 层方式实现感知机神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成样本数据，样本容量4，特征数784\n",
    "x = tf.random.normal([4, 784])\n",
    "\n",
    "# 创建一个全连接层，神经元个数512，使用relu作为激活函数\n",
    "fc = tf.keras.layers.Dense(512, activation = 'relu')\n",
    "\n",
    "# 让x通过这个层\n",
    "h1 = fc(x)\n",
    "h1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([784, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取这个全连接层的权值矩阵，因为有784个特征和512个神经元，所以形状是784*512\n",
    "fc.kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取这个全连接层的偏执向量，因为有512个神经元，所以有512个bias\n",
    "fc.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([784, 512]), TensorShape([512]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得待优化参数的列表，也就是权值矩阵和偏置向量\n",
    "fc.trainable_variables[0].shape, fc.trainable_variables[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fc.variables返回所有参数列表，有一些不同于全连接层，可能存在不可优化的参数\n",
    "\n",
    "利用网络层类对象进行前向计算时，只需要调用类的\\__call__方法即可，即写成fc(x)方式，它会自动调用类的\\__call__方法，在\\__call__方法中自动调用call方法，完成σ(X@W+b)的运算逻辑，最后返回全连接层的输出张量。\n",
    "\n",
    "### 4. 神经网络\n",
    "通过层层堆叠全连接层，保证前一层的输出节点数与当前层的输入节点数匹配，即可堆叠出任意层数的网络。我们把这种由神经元构成的网络叫做神经网络。\n",
    "\n",
    "通过堆叠4个全连接层，可以获得层数为4的神经网络，由于每层均为全连接层，称为全连接网络。其中第1~3个全连接层在网络中间，称之为隐藏层1,2,3,最后一个全连接层的输出作为网络的输出，称为输出层。\n",
    "\n",
    "#### 4.1 张量方式实现多层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 256]),\n",
       " TensorShape([4, 128]),\n",
       " TensorShape([4, 64]),\n",
       " TensorShape([4, 10]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([4, 784])\n",
    "\n",
    "# 隐藏层1\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev = 0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "\n",
    "# 隐藏层2\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev = 0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "\n",
    "# 隐藏层3\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 64], stddev = 0.1))\n",
    "b3 = tf.Variable(tf.zeros([64]))\n",
    "\n",
    "# 输出层\n",
    "wo = tf.Variable(tf.random.truncated_normal([64, 10], stddev = 0.1))\n",
    "bo = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 开始计算\n",
    "out1 = tf.matmul(x, w1) + b1\n",
    "out1 = tf.nn.relu(out1)\n",
    "out2 = tf.matmul(out1, w2) + b2\n",
    "out2 = tf.nn.relu(out2)\n",
    "out3 = tf.matmul(out2, w3) + b3\n",
    "out3 = tf.nn.relu(out3)\n",
    "# 输出层可以添加激活函数，也可以不添加，视情况而定\n",
    "out = tf.matmul(out3, wo) + bo\n",
    "\n",
    "out1.shape, out2.shape, out3.shape, out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 层方法实现多层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 256]),\n",
       " TensorShape([4, 128]),\n",
       " TensorShape([4, 64]),\n",
       " TensorShape([4, 10]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([4, 756])\n",
    "\n",
    "# 隐藏层1\n",
    "fc1 = tf.keras.layers.Dense(256, activation = 'relu')\n",
    "\n",
    "# 隐藏层2\n",
    "fc2 = tf.keras.layers.Dense(128, activation = 'relu')\n",
    "\n",
    "# 隐藏层3\n",
    "fc3 = tf.keras.layers.Dense(64, activation = 'relu')\n",
    "\n",
    "# 输出层\n",
    "fc4 = tf.keras.layers.Dense(10)\n",
    "\n",
    "# 开始计算\n",
    "h1 = fc1(x)\n",
    "h2 = fc2(h1)\n",
    "h3 = fc3(h2)\n",
    "out = fc4(h3)\n",
    "\n",
    "h1.shape, h2.shape, h3.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Sequential容器封装成一个网络大类对象\n",
    "# 调用大类的前向计算函数可完成所有层的计算\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# 开始计算\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 目标优化\n",
    "- 我们把神经网络从输入到输出的计算过程叫做前向传播。神经网络的前向传播过程，也是数据张量从第一层流动至输出层的过程：从输入数据开始，途径每个隐藏层，直至得到输出并计算误差，这也是Tensorflow框架名字意义所在。前向传播的最后一步就是完成误差的计算。\n",
    "\n",
    "- 利用误差反向传播算法进行反向计算的过程也叫反向传播。从另一个角度来理解神经网络，它完成的是特征的维度变换的功能，比如4层的MINIST手写数字图片识别的全连接网络，它依次完成了784→256→128→64→10的特征降维过程。原始的特征通常具有较高的维度，包含了很多底层特征及无用信息，通过神经网络的层层特征变换，将较高的维度降维到较低的维度，此时的特征一般包含了与任务强相关的高层特征信息，通过对这些特征进行简单的逻辑判定即可完成特定的任务，如图片的分类。\n",
    "\n",
    "#### 参数量\n",
    "- 网络的参数量是衡量网络规模的重要指标。考虑权值矩阵w，偏置b，输入特征长度为din，输出特征长度为dout的网络层，其参数量为din\\*dout，再加上偏置b的参数，总参数量为din\\*dout+dout。\n",
    "- 对于多层的全连接神经网络，比如784→256→128→64→10,总参数量计算表达式256\\*784+256+128\\*256+128+64\\*128+64+10\\*64+10=242762约242K个参数量。\n",
    "\n",
    "### 6. 激活函数\n",
    "#### 6.1 Sigmoid函数\n",
    "- 它的一个优良特性就是能够把x∈R的输入“压缩”到x∈[0,1]区间。\n",
    "- 概率分布[0,1]区间的输出和概率的分布范围契合，可以通过Sigmoid函数将输出转译为概率输出。\n",
    "- 信号强度一般可以将0~1理解为某种信号的强度，如像素的颜色强度，1代表当前通道颜色最强，0代表当前通道无颜色：抑或代表门控值（Gate）的强度，1代表当前门控全部开放，0代表门控关闭。\n",
    "- Sigmoid函数连续可导，相对于阶跃函数，可以直接利用梯度下降算法优化网络参数，应用的非常广泛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1127, shape=(10,), dtype=float32, numpy=\n",
       "array([0.00247264, 0.00931591, 0.03444517, 0.11920291, 0.33924365,\n",
       "       0.6607564 , 0.8807971 , 0.96555483, 0.99068403, 0.9975274 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用sigmoid函数\n",
    "x = tf.linspace(-6., 6, 10)\n",
    "tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 ReLU函数\n",
    "- 在ReLU(Rectified Linear Unit，修正线性单元)激活函数提出之前， Sigmoid函数通常是神经网络的激活函数首选。但是Sigmoid函数在输入值较大或较小时容易出现梯度值接近于0的现象，称为梯度弥散现象，网络参数长时间得不到更新，很难训练较深层次的网络模型。\n",
    "- ReLU对小于0的值全部抑制为0，对于正数则直接输出，这种单边抑制性来源于生物学。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1128, shape=(10,), dtype=float32, numpy=\n",
       "array([0.      , 0.      , 0.      , 0.      , 0.      , 0.666667,\n",
       "       2.      , 3.333334, 4.666667, 6.      ], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用ReLU函数\n",
    "tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1227, shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 2.0215042 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19366178, 0.        , 1.4337869 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.05794102, 0.        , 0.        ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以将ReLU函数作为一层嵌入神经网络中\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.ReLU() # 嵌入ReLU层，相当于在隐藏层1使用了ReLU作为激活函数\n",
    "])\n",
    "\n",
    "x = tf.random.normal([2, 756])\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 LeakyReLU\n",
    "- ReLU函数在x<0时梯度值恒为0,也可能会造成梯度弥散现象，为了克服这个题，Leakyrelu函数被提出。\n",
    "- LeakyReLU函数在输入x<0时，也会有一个较小的输出，其梯度较小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1247, shape=(10,), dtype=float32, numpy=\n",
       "array([-1.2       , -0.93333334, -0.6666667 , -0.4       , -0.13333331,\n",
       "        0.666667  ,  2.        ,  3.333334  ,  4.666667  ,  6.        ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用LeakyReLU函数\n",
    "x = tf.linspace(-6., 6., 10)\n",
    "tf.nn.leaky_relu(x) # 斜率alpha默认为0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过tf.keras.layers.LeakyReLU()也可以将其作为层嵌入神经网络\n",
    "#### 6.4 Tanh函数\n",
    "- tanh函数可通过Sigmoid函数缩放平移后实现，压缩至[-1, 1]区间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1248, shape=(10,), dtype=float32, numpy=\n",
       "array([-0.99998784, -0.99982315, -0.9974579 , -0.9640276 , -0.58278286,\n",
       "        0.58278316,  0.9640276 ,  0.99745804,  0.99982315,  0.99998784],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Tanh函数\n",
    "tf.nn.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 输出层设计\n",
    "网络的最后一层除了和所有的隐藏层一样，完成维度变换、特征提取的功能，还作为输出层使用，需要根据具体的任务场景来决定是否使用激活函数，以及使用什么类型的激活函数。常见的几种输出类型包括：\n",
    "- out∈R^d，输出属于整个实数空间，或者某段普通的实数空间，比如函数值趋势的预测年龄的预测问题等。\n",
    "- out∈[0,1]，输出值特别地落在[0,1]的区间，如图片生成，图片像素值一般用[0,1]表示；或者二分类问题的概率，如硬币正反面的概率预测问题。\n",
    "- out∈[0,1]，并且所有输出值之和为1，常见的如多分类问题，如MNIST手写数字图片识别，图片属于10个类别的概率之和为1。\n",
    "- out∈[-1,1]。\n",
    "\n",
    "#### 7.1 out∈R^d\n",
    "这一类问题比较普遍，像正弦函数曲线预测、年龄的预测、股票走势的预测等都属于整个或者部分连续的实数空间，输出层可以不加激活函数。误差的计算直接基于最后一层的输出和真实值y进行计算，如采用均方差误差函数度量输出值o与真实值y之间的距离。\n",
    "\n",
    "#### 7.2 out∈[0,1]\n",
    "输出值属于[0,1]区间也比较常见，比如图片的生成，二分类问题等。在机器学习中一般会将图片的像素值归一化到[0,1]区间。为了让像素的值范围映射到[0,1]的有效实数空间，需要在输出层后添加某个合适的激活函数,其中Sigmoid函数刚好具有此功能。\n",
    "\n",
    "同样的，对于二分类问题，如硬币的正反面的预测，输出层可以只需要一个节点，表示某个事件A发生的概率P(A|x)，只需要在输出层的净活性值z后添加Sigmoid函数即可将输出转译为概率值。\n",
    "\n",
    "#### 7.3 out∈[0,1]，和为1\n",
    "这种设定以多分类问题最为常见。可以通过在输出层添加 Softmax函数实现。\n",
    "\n",
    "在Softmax函数的数值计算过程中，容易因输入值偏大发生数值溢出现象；在计算交叉熵时，也会出现数值溢出的问题。为了数值计算的稳定性，Tensorflow中提供了一个统一的接口，将Softmax与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，推荐使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1250, shape=(3,), dtype=float32, numpy=array([0.6590012 , 0.24243298, 0.09856589], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用softmax函数\n",
    "x = tf.constant([2., 1., 0.1])\n",
    "tf.nn.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=1514, shape=(), dtype=float32, numpy=2.1327703>,\n",
       " <tf.Tensor: id=1516, shape=(), dtype=float32, numpy=2.1327705>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras.losses.categorical_crossentropy\n",
    "# 构造输出层输出，2个样本，每个样本10个输出，表示每个类的可能性\n",
    "z = tf.random.normal([2, 10])\n",
    "z_softmax = tf.nn.softmax(z)\n",
    "\n",
    "# 构造真实值，比如两个样本类为1和3\n",
    "y_onehot = tf.constant([1, 3])\n",
    "\n",
    "# 进行one-hot编码，depth表示一共有10个类\n",
    "y_onehot = tf.one_hot(y_onehot, depth = 10) \n",
    "\n",
    "# 计算交叉熵，第一个参数是真实值，第二个参数是预测值\n",
    "# from_logits=True表示输出层未使用softmax函数，为False表示使用了softmax函数\n",
    "loss = tf.keras.losses.categorical_crossentropy(y_onehot, z, from_logits = True)\n",
    "loss_softmax = tf.keras.losses.categorical_crossentropy(y_onehot, z_softmax, \n",
    "                                                       from_logits = False)\n",
    "\n",
    "# 计算平均交叉熵损失\n",
    "loss = tf.reduce_mean(loss)\n",
    "loss_softmax = tf.reduce_mean(loss_softmax)\n",
    "loss, loss_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1566, shape=(), dtype=float32, numpy=2.1327703>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# losses.CategoricalCrossentropy可以实现交叉熵计算和softmax函数计算\n",
    "criteon = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "loss = criteon(y_onehot, z)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 out∈[-1, 1]\n",
    "可以简单的使用tanh激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1571, shape=(10,), dtype=float32, numpy=\n",
       "array([-0.99998784, -0.99982315, -0.9974579 , -0.9640276 , -0.58278286,\n",
       "        0.58278316,  0.9640276 ,  0.99745804,  0.99982315,  0.99998784],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.linspace(-6., 6., 10)\n",
    "tf.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 误差计算\n",
    "- 均方差\n",
    "- 交叉熵\n",
    "- KL散度\n",
    "- Hinge Loss函数\n",
    "- ……\n",
    "\n",
    "#### 8.1 均方差(MSE)\n",
    "MSE误差函数的值总是大于等于0,当MSE函数达到最小值0时，输出等于真实标签\n",
    "此时神经网络的参数达到最优状态。\n",
    "\n",
    "均方差广泛应用在回归问题中，在分类问题中也可以应用均方差误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1601, shape=(), dtype=float32, numpy=0.89064753>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE计算\n",
    "out = tf.random.normal([2,10])\n",
    "y_onehot = tf.constant([1,3])\n",
    "y_onehot = tf.one_hot(y_onehot, depth = 10)\n",
    "\n",
    "# tf.keras.losses.MSE计算均方差，得到每个样本的均方差\n",
    "loss = tf.keras.losses.MSE(y_onehot, out)\n",
    "\n",
    "# 再次计算平均值得到整个batch的均方差\n",
    "loss = tf.reduce_mean(loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1623, shape=(), dtype=float32, numpy=0.89064753>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以通过层的方式实现\n",
    "criteon = tf.keras.losses.MeanSquaredError()\n",
    "loss = criteon(y_onehot, out)\n",
    "loss = tf.reduce_mean(loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 交叉熵\n",
    "交叉熵可以很好的衡量2个分布之间的差别，特别当分类问题中y的编码分布p采用one-hot编码时。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
